<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  
  <!-- Favicon code from realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="/owenhuang.github.io/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="96x96" href="/owenhuang.github.io/favicon-96x96.png">
<link rel="manifest" href="/owenhuang.github.io/site.webmanifest">
<link rel="mask-icon" href="/owenhuang.github.io/safari-pinned-tab.svg">
<!-- <meta name="msapplication-TileColor" content="#563d7c">
<meta name="theme-color" content="#ffffff"> -->

  <!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>

  <!-- CSS -->
  <link rel="stylesheet" href="/owenhuang.github.io/assets/css/main.css">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Yidong Huang</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Yidong Huang" />
<meta name="author" content="Yidong Huang" />
<meta property="og:locale" content="en_GB" />
<meta name="description" content="Yidong Huang" />
<meta property="og:description" content="Yidong Huang" />
<link rel="canonical" href="https://h6kplus.github.io/owenhuang.github.io/" />
<meta property="og:url" content="https://h6kplus.github.io/owenhuang.github.io/" />
<meta property="og:site_name" content="Yidong Huang" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Yidong Huang" />
<meta name="twitter:site" content="@owenhunag117" />
<meta name="twitter:creator" content="@Yidong Huang" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"Yidong Huang"},"description":"Yidong Huang","headline":"Yidong Huang","name":"Yidong Huang","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://h6kplus.github.io/owenhuang.github.io/assets/img/avatar.jpg"},"name":"Yidong Huang"},"sameAs":["https://twitter.com/owenhuang117","https://github.com/h6kplus"],"url":"https://h6kplus.github.io/owenhuang.github.io/"}</script>
<!-- End Jekyll SEO tag -->

</head>
<!--jQuery-->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<body>
  <div class="container">
    

<section id="header-nav">
  <header>
    <nav>
      <ul>
        
        <li class="current">Home</li>
        
          <a href="/owenhuang.github.io/publications">
            <li class="btn-nav">Publications</li>
          </a>
        
        
          <a href="/owenhuang.github.io/presentations">
            <li class="btn-nav">Presentations</li>
          </a>
        
        
          <a href="/owenhuang.github.io/blog">
            <li class="btn-nav">Blog</li>
          </a>
          <a href="/owenhuang.github.io/tags">
            <li class="btn-nav">Tags</li>
          </a>
        
        <!-- publications -->
        
      </ul>
    </nav>
  </header>
</section>




</div><!-- This Tag means End .container -->
<div id="portfolio">
  <section id="about-me">
    <div class="row">
      <div class="img-wrap col-12 col-md-4">
        <div class="img">
          <img src="/owenhuang.github.io/assets/img/avatar.jpg" alt="user_image">
        </div>
      </div>
      <div class="text-wrap col-12 col-md-8">
        <h2 class="username">Yidong Huang</h2>
        <div class="description"><p>I’m a incoming CS Ph.D. student at UNC Chapel Hill, advised by <a href="https://www.cs.unc.edu/~mbansal/">Prof. Mohit Bansal</a>. I obtained my master degree from University of Michigan advised by <a href="https://web.eecs.umich.edu/~chaijy/">Prof. Joyce Chai</a>. In my master time, I also interned at Boson AI, supervised by <a href="https://bryanyzhu.github.io/">Yi Zhu</a>. Before that, I got my bachelor degree from University of Michigan and Shanghai Jiao Tong University.</p>
</div>  
        <div class="interest"><p>I specialize in <u>Embodied Artificial Intelligence</u> and the <u>Generative AI</u> that interact with humans and their environments. My current research goal is to create AI agents beyond mere perception and reactive generation, to develop rich representations of the world and human partners, enabling deliberative planning and collaboration with humans.</p>
</div>
        <div class="outside-work"><p>Beyond work, I enjoy all kinds of sports, building and playing games, watching animations, and connecting with people from diverse backgrounds. If we share any interests, feel free to reach out!</p>
</div>
        <div class="call-for-work"><p>I’m always open to research collaborations, project ideas, or just a good conversation. Feel free to contact me via email!</p>
</div>

        <ul id="icons">
          

<li>
  <a href="mailto:%6F%77%65%6E%68@%63%73.%75%6E%63.%65%64%75" target="_blank" title="Email">
    <i class="fas fa-envelope"></i>
  </a>
</li>





<li>
  <a href="https://scholar.google.com/citations?user=SdEBnsoAAAAJ&hl" target="_blank" title="Google Scholar">
    <i class="ai ai-google-scholar"></i>
  </a>
</li>











<li>
  <a href="https://www.linkedin.com/in/yidong-huang-61a86423b/" target="_blank" title="Linkedin">
    <i class="fab fa-linkedin"></i>
  </a>
</li>



<li>
  <a href="https://github.com/h6kplus" target="_blank" title="Github">
    <i class="fab fa-github"></i>
  </a>
</li>











<li>
  <a href="https://twitter.com/owenhuang117" target="_blank" title="Twitter">
    <i class="fab fa-twitter"></i>
  </a>
</li>







        </ul>
      </div>
    </div>
    <div class="bg"></div>
  </section>
  <!-- career section -->
  <section id="career" class="row">
    <article class="exper col-12 col-md-6">
      
<p class="title">EDUCATION</p>
<ul class="exper-list">
    
    <li class="meta">
        <p class="date">2025-08-01 &ndash; </p>
        <p class="description">The University of North Carolina at Chapel Hill</p>
        <p class="program">Ph.D. in Computer Science</p>
    </li>
    
    <li class="meta">
        <p class="date">2023-09-01 &ndash; 2025-04-31</p>
        <p class="description">University of Michigan</p>
        <p class="program">MS in Computer Science</p>
    </li>
    
    <li class="meta">
        <p class="date">2021-09-01 &ndash; 2023-04-30</p>
        <p class="description">University of Michigan</p>
        <p class="program">B.S.E in Computer Science</p>
    </li>
    
    <li class="meta">
        <p class="date">2019-09-01 &ndash; 2023-08-01</p>
        <p class="description">Shanghai Jiao Tong Univeristy</p>
        <p class="program">B.S.E in Electronic and Computer Engineering</p>
    </li>
    
    <li class="meta">
        <p class="date">2016-09-01 &ndash; 2019-06-30</p>
        <p class="description">No. 2 High School of East China Normal University</p>
        <p class="program"></p>
    </li>
    
</ul>

    </article>
    <article class="skills col-12 col-md-6">
      
<p class="title">RESEARCH EXPERIENCE</p>
<ul class="interests-list">
    
    <li class="meta">
        <p class="description">MURGe-Lab (UNC-NLP Group)</p>
        <p class="program">Graduate Research Assistant</p>
    </li>
    
    <li class="meta">
        <p class="description">Boson AI</p>
        <p class="program">Research Intern</p>
    </li>
    
    <li class="meta">
        <p class="description">Situated Language and Embodied Dialogue (SLED) lab</p>
        <p class="program">Research Assistant</p>
    </li>
    
</ul>

      
  <p class="title">RESEARCH INTERESTS</p>
  <ul class="awards-list">
    
    <li class="award">Embodied AI</li>
    
    <li class="award">Generative AI</li>
    
    <li class="award">Multimodal Learning</li>
    
  </ul>


    </article>
  </section>

  <section id="project">
  <div class="project-wrap">
    <p class="title">PROJECTS</p>
    <div class="card-container">
      
      
      
      <div class="card-wrapper">
        <div class="card">
          
          <div class="img-wrap">
            <img src="/owenhuang.github.io/assets/project/drivlme.jpg" alt="">
          </div>
          
          <div class="text-wrap ">
            <div class="title-wrap">
              <p class="date">10/2023 &ndash; 03/2024</p>
              <div class="title">DriVLMe, Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences</div>
            </div>
            <div class="meta">
              
            </div>
            <div class="description">Recent advancements in foundation models (FMs) have unlocked new prospects in autonomous driving, yet the experimental settings of these studies are preliminary, over-simplified, and fail to capture the complexity of real-world driving scenarios in human environments. It remains under-explored whether FM agents can handle long-horizon navigation tasks with free-from dialogue and deal with unexpected situations caused by environmental dynamics or task changes. To explore the capabilities and boundaries of FMs faced with the challenges above, we introduce DriVLMe, a video-language-model-based agent to facilitate natural and effective communication between humans and autonomous vehicles that perceive the environment and navigate. We develop DriVLMe from both embodied experiences in a simulated environment and social experiences from real human dialogue. While DriVLMe demonstrates competitive performance in both open-loop benchmarks and closed-loop human studies, we reveal several limitations and challenges, including unacceptable inference time, imbalanced training data, limited visual understanding, challenges with multi-turn interactions, simplified language generation from robotic experiences, and difficulties in handling on-the-fly unexpected situations like environmental dynamics and task changes.</div>
          </div>
          <div class="icons">
            
            
            <a href="https://sled-group.github.io/driVLMe/"><i class="far fa-eye"></i></a>
            
            
            <a title="1" onClick="open_modal(this.title)" class="modal-btn"><i class="fas fa-plus"></i></a>
            
          </div>
        </div>
      </div>
      <!-- Modal Box -->
      
      <div id="1" class="modal">
  <div class="modal-background" onClick="close_modal()"></div>
  <div class="modal-content">
    <div class="close-modal" onClick="close_modal()">
      <p>&times;</p>
    </div>
    <div class="header">
      <p class="date">10/2023 &ndash; 03/2024</p>
      <p class="title">DriVLMe, Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences</p>
      <p class="summary">https://arxiv.org/abs/2406.03008</p>
    </div>
    <div class="content">
      
        <img src="/owenhuang.github.io/assets/project/drivlme.jpg" alt="">
      
      <div class="description">Recent advancements in foundation models (FMs) have unlocked new prospects in autonomous driving, yet the experimental settings of these studies are preliminary, over-simplified, and fail to capture the complexity of real-world driving scenarios in human environments. It remains under-explored whether FM agents can handle long-horizon navigation tasks with free-from dialogue and deal with unexpected situations caused by environmental dynamics or task changes. To explore the capabilities and boundaries of FMs faced with the challenges above, we introduce DriVLMe, a video-language-model-based agent to facilitate natural and effective communication between humans and autonomous vehicles that perceive the environment and navigate. We develop DriVLMe from both embodied experiences in a simulated environment and social experiences from real human dialogue. While DriVLMe demonstrates competitive performance in both open-loop benchmarks and closed-loop human studies, we reveal several limitations and challenges, including unacceptable inference time, imbalanced training data, limited visual understanding, challenges with multi-turn interactions, simplified language generation from robotic experiences, and difficulties in handling on-the-fly unexpected situations like environmental dynamics and task changes.</div>
      
    </div>
  </div>
</div>
      
      <!-- Modal Box -->
      
      
      
      <div class="card-wrapper">
        <div class="card">
          
          <div class="img-wrap">
            <img src="/owenhuang.github.io/assets/project/infedit_gif.gif" alt="">
          </div>
          
          <div class="text-wrap ">
            <div class="title-wrap">
              <p class="date">09/2022 &ndash; 11/2023</p>
              <div class="title">Inversion-Free Image Editing with Natural Language</div>
            </div>
            <div class="meta">
              
            </div>
            <div class="description">Despite recent advances in inversion-based editing, text-guided image manipulation remains challenging for diffusion models. The primary bottlenecks include 1) the time-consuming nature of the inversion process; 2) the struggle to balance consistency with accuracy; 3) the lack of compatibility with efficient consistency sampling methods used in consistency models. To address the above issues, we start by asking ourselves if the inversion process can be eliminated for editing. We show that when the initial sample is known, a special variance schedule reduces the denoising step to the same form as the multi-step consistency sampling. We name this Denoising Diffusion Consistent Model (DDCM), and note that it implies a virtual inversion strategy without explicit inversion in sampling. We further unify the attention control mechanisms in a tuning-free framework for text-guided editing. Combining them, we present inversion-free editing (InfEdit), which allows for consistent and faithful editing for both rigid and non-rigid semantic changes, catering to intricate modifications without compromising on the image's integrity and explicit inversion. Through extensive experiments, InfEdit shows strong performance in various editing tasks and also maintains a seamless workflow (less than 3 seconds on one single A40), demonstrating the potential for real-time applications.</div>
          </div>
          <div class="icons">
            
            
            <a href="https://sled-group.github.io/InfEdit/"><i class="far fa-eye"></i></a>
            
            
            <a title="2" onClick="open_modal(this.title)" class="modal-btn"><i class="fas fa-plus"></i></a>
            
          </div>
        </div>
      </div>
      <!-- Modal Box -->
      
      <div id="2" class="modal">
  <div class="modal-background" onClick="close_modal()"></div>
  <div class="modal-content">
    <div class="close-modal" onClick="close_modal()">
      <p>&times;</p>
    </div>
    <div class="header">
      <p class="date">09/2022 &ndash; 11/2023</p>
      <p class="title">Inversion-Free Image Editing with Natural Language</p>
      <p class="summary">https://arxiv.org/abs/2312.04965</p>
    </div>
    <div class="content">
      
        <img src="/owenhuang.github.io/assets/project/infedit_gif.gif" alt="">
      
      <div class="description">Despite recent advances in inversion-based editing, text-guided image manipulation remains challenging for diffusion models. The primary bottlenecks include 1) the time-consuming nature of the inversion process; 2) the struggle to balance consistency with accuracy; 3) the lack of compatibility with efficient consistency sampling methods used in consistency models. To address the above issues, we start by asking ourselves if the inversion process can be eliminated for editing. We show that when the initial sample is known, a special variance schedule reduces the denoising step to the same form as the multi-step consistency sampling. We name this Denoising Diffusion Consistent Model (DDCM), and note that it implies a virtual inversion strategy without explicit inversion in sampling. We further unify the attention control mechanisms in a tuning-free framework for text-guided editing. Combining them, we present inversion-free editing (InfEdit), which allows for consistent and faithful editing for both rigid and non-rigid semantic changes, catering to intricate modifications without compromising on the image's integrity and explicit inversion. Through extensive experiments, InfEdit shows strong performance in various editing tasks and also maintains a seamless workflow (less than 3 seconds on one single A40), demonstrating the potential for real-time applications.</div>
      
    </div>
  </div>
</div>
      
      <!-- Modal Box -->
      
      
      
      <div class="card-wrapper">
        <div class="card">
          
          <div class="img-wrap">
            <img src="/owenhuang.github.io/assets/project/cyclenet.png" alt="">
          </div>
          
          <div class="text-wrap ">
            <div class="title-wrap">
              <p class="date">03/2023 &ndash; 06/2023</p>
              <div class="title">CycleNet, Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation</div>
            </div>
            <div class="meta">
              
            </div>
            <div class="description">Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate Cyclenet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that Cyclenet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual prompt. Cyclenet is a practical framework, which is robust even with very limited training data (around 2k) and requires minimal computational resources (1 GPU) to train.</div>
          </div>
          <div class="icons">
            
            
            <a href="https://cyclenetweb.github.io/"><i class="far fa-eye"></i></a>
            
            
            <a title="3" onClick="open_modal(this.title)" class="modal-btn"><i class="fas fa-plus"></i></a>
            
          </div>
        </div>
      </div>
      <!-- Modal Box -->
      
      <div id="3" class="modal">
  <div class="modal-background" onClick="close_modal()"></div>
  <div class="modal-content">
    <div class="close-modal" onClick="close_modal()">
      <p>&times;</p>
    </div>
    <div class="header">
      <p class="date">03/2023 &ndash; 06/2023</p>
      <p class="title">CycleNet, Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation</p>
      <p class="summary">Admitted to NeurIPS 2023</p>
    </div>
    <div class="content">
      
        <img src="/owenhuang.github.io/assets/project/cyclenet.png" alt="">
      
      <div class="description">Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate Cyclenet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that Cyclenet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual prompt. Cyclenet is a practical framework, which is robust even with very limited training data (around 2k) and requires minimal computational resources (1 GPU) to train.</div>
      
    </div>
  </div>
</div>
      
      <!-- Modal Box -->
      
      
      
      <div class="card-wrapper">
        <div class="card">
          
          <div class="img-wrap">
            <img src="/owenhuang.github.io/assets/project/dorothie.png" alt="The Double Wizard Setup">
          </div>
          
          <div class="text-wrap ">
            <div class="title-wrap">
              <p class="date">02/2022 &ndash; 06/2022</p>
              <div class="title">DOROTHIE, Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents</div>
            </div>
            <div class="meta">
              
            </div>
            <div class="description">We tackled the limitations of vision-language navigation tasks, where most existing approaches were limited in their ability to navigate in a continuous and dynamic environment and communicate with humans in free-form. To address these limitations and collect data, we extended the traditional Wizard of Oz study and proposed the duo-wizard setup. This allowed us to add dynamic changes in the environment and tasks to the simulation, which provided a more realistic testbed for evaluating an agent's ability to communicate and navigate.</div>
          </div>
          <div class="icons">
            
            
            <a href="https://arxiv.org/abs/2210.12511"><i class="far fa-eye"></i></a>
            
            
            <a title="4" onClick="open_modal(this.title)" class="modal-btn"><i class="fas fa-plus"></i></a>
            
          </div>
        </div>
      </div>
      <!-- Modal Box -->
      
      <div id="4" class="modal">
  <div class="modal-background" onClick="close_modal()"></div>
  <div class="modal-content">
    <div class="close-modal" onClick="close_modal()">
      <p>&times;</p>
    </div>
    <div class="header">
      <p class="date">02/2022 &ndash; 06/2022</p>
      <p class="title">DOROTHIE, Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents</p>
      <p class="summary">Admitted to findings of EMNLP 2022</p>
    </div>
    <div class="content">
      
        <img src="/owenhuang.github.io/assets/project/dorothie.png" alt="The Double Wizard Setup">
      
      <div class="description">We tackled the limitations of vision-language navigation tasks, where most existing approaches were limited in their ability to navigate in a continuous and dynamic environment and communicate with humans in free-form. To address these limitations and collect data, we extended the traditional Wizard of Oz study and proposed the duo-wizard setup. This allowed us to add dynamic changes in the environment and tasks to the simulation, which provided a more realistic testbed for evaluating an agent's ability to communicate and navigate.</div>
      
    </div>
  </div>
</div>
      
      <!-- Modal Box -->
      
      
      
      <div class="card-wrapper">
        <div class="card">
          
          <div class="img-wrap">
            <img src="/owenhuang.github.io/assets/project/aesrgan.png" alt="">
          </div>
          
          <div class="text-wrap ">
            <div class="title-wrap">
              <p class="date">11/2021 &ndash; 12/2021</p>
              <div class="title">A-ESRGAN, Training Real-World Blind Super-Resolution with Attention U-Net Discriminators</div>
            </div>
            <div class="meta">
              
            </div>
            <div class="description">In the field of Computer Vision, my work focuses on a novel approach to Blind Image Super-Resolution (SR), a task aimed at restoring low-resolution images affected by complex, unknown distortions. My key contribution is the development of A-ESRGAN, an innovative Generative Adversarial Network (GAN) model featuring an attention U-Net based, multi-scale discriminator. This model stands out as the first to integrate attention U-Net structure as a discriminator in GAN for addressing blind SR challenges.
My research addresses the limitations of existing GAN structures that neglect an image's structural features, leading to issues like twisted lines and background anomalies. A-ESRGAN overcomes these through its unique design, enabling enhanced focus on structural details across multiple scales. The result is a breakthrough in generating more perceptually realistic high-resolution images.
This work not only sets a new benchmark in the non-reference natural image quality evaluator (NIQE) metric but also demonstrates, through extensive ablation studies, how the RRDB-based generator in A-ESRGAN effectively leverages image structural features, outperforming previous models in blind SR tasks.</div>
          </div>
          <div class="icons">
            
            
            <a href="https://arxiv.org/abs/2112.10046"><i class="far fa-eye"></i></a>
            
            
            <a title="5" onClick="open_modal(this.title)" class="modal-btn"><i class="fas fa-plus"></i></a>
            
          </div>
        </div>
      </div>
      <!-- Modal Box -->
      
      <div id="5" class="modal">
  <div class="modal-background" onClick="close_modal()"></div>
  <div class="modal-content">
    <div class="close-modal" onClick="close_modal()">
      <p>&times;</p>
    </div>
    <div class="header">
      <p class="date">11/2021 &ndash; 12/2021</p>
      <p class="title">A-ESRGAN, Training Real-World Blind Super-Resolution with Attention U-Net Discriminators</p>
      <p class="summary">Admitted to PRICAI 2022</p>
    </div>
    <div class="content">
      
        <img src="/owenhuang.github.io/assets/project/aesrgan.png" alt="">
      
      <div class="description">In the field of Computer Vision, my work focuses on a novel approach to Blind Image Super-Resolution (SR), a task aimed at restoring low-resolution images affected by complex, unknown distortions. My key contribution is the development of A-ESRGAN, an innovative Generative Adversarial Network (GAN) model featuring an attention U-Net based, multi-scale discriminator. This model stands out as the first to integrate attention U-Net structure as a discriminator in GAN for addressing blind SR challenges.
My research addresses the limitations of existing GAN structures that neglect an image's structural features, leading to issues like twisted lines and background anomalies. A-ESRGAN overcomes these through its unique design, enabling enhanced focus on structural details across multiple scales. The result is a breakthrough in generating more perceptually realistic high-resolution images.
This work not only sets a new benchmark in the non-reference natural image quality evaluator (NIQE) metric but also demonstrates, through extensive ablation studies, how the RRDB-based generator in A-ESRGAN effectively leverages image structural features, outperforming previous models in blind SR tasks.</div>
      
    </div>
  </div>
</div>
      
      <!-- Modal Box -->
      
    </div>
  </div>
  <div class="bg"></div>
</section>

<!-- modal -->
<script type="text/javascript" src="/owenhuang.github.io/assets/js/modal.js"></script>

  <section id="recentpubs">
  <div class="recentpubs-wrap">
    <p class="title">RECENT PUBLICATIONS</p>
    [<a class="btn" href="/owenhuang.github.io/publications" title="Publications">All Publications</a>]
    <ol class="bibliography"><li><!-- Entry bib key -->
<div id="huang2024drivlme">
  
  <!-- Title -->
  <div class="title">Drivlme: Enhancing llm-based autonomous driving agents with embodied and social experiences</div>
  <!-- Author -->
  <div class="author">Huang, Yidong,&nbsp;Sansom, Jacob,&nbsp;Ma, Ziqiao,&nbsp;Gervits, Felix,&nbsp;and Chai, Joyce
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 3153–3160, 2024
  </div>
</div></li>
<li><!-- Entry bib key -->
<div id="wei2023esrgan">
  
  <!-- Title -->
  <div class="title">A-ESRGAN: Training real-world blind super-resolution with attention U-Net Discriminators</div>
  <!-- Author -->
  <div class="author">Wei, Zihao,&nbsp;Huang, Yidong,&nbsp;Chen, Yuang,&nbsp;Zheng, Chenhao,&nbsp;and Gao, Jingnan
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In Pacific Rim International Conference on Artificial Intelligence, pp. 16–27, 2023
  </div>
</div></li>
<li><!-- Entry bib key -->
<div id="xu2023cyclenet">
  
  <!-- Title -->
  <div class="title">CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation</div>
  <!-- Author -->
  <div class="author">Xu, Sihan,&nbsp;Ma, Ziqiao,&nbsp;Huang, Yidong,&nbsp;Lee, Honglak,&nbsp;and Chai, Joyce
  </div>

  <!-- Journal/Book title and date -->
  
  <div class="third-line">
    In Thirty-seventh Conference on Neural Information Processing Systems, 2023
  </div>
</div></li></ol>
  </div>
</section>
</div>

<!-- Footer -->
<footer>
  <div class="footer">
    Copyright © 2022
    <a href=""></a>.
    Powered by Jekyll with
    <a href="https://github.com/chrjabs/Grape-Academic-Theme">Grape Academic Theme</a>.
  </div>
</footer>

  </div>
</body>

</html>